)
variance = var(sim)
sample_var[i] = variance
for (i in 1:trials) {
sim = rnorm(7, 4, sqrt(6))
variance = var(sim)
sample_var[i] = variance
}
d = density(sample_var)
plot(
d,
lwd=2,
main="Distribution Plot, Chi-Squared",
xlab='x')
curve(
dchisq(x, 6),
col='red',
add=TRUE,
lwd=2)
legend(
x="topright",
legend=c(expression(frac((n-1)*S^2, sigma^2)), expression(chi[n-1]^2)),
col=c('black', 'red'),
lty=c(1, 1),
lwd=c(2, 2)
)
# Q 74
discrete_data = 1:10
?dgeom
dgeom(discrete_data, prob=0.75)
discrete_data - 1
dgeom(discrete_data - 1, prob=0.75)
geom = dgeom(discrete_data - 1, prob=0.75)
plot(geom)
yulesimon = 2*2*factorial(discrete_data - 1)
yulesimon = 2*2*factorial(discrete_data - 1) / factorial(discrete_data+2)
plot(yulesimon)
plot(geom)
?plot
plot(geom, pch=10)
plot(geom, pch=19)
plot(yulesimon, pch=19, col='red')
points(yulesimon, pch=19, col='red')
plot(geom, pch=19)
points(yulesimon, pch=19, col='red')
plot(geom, pch=19, ylab='probability')
points(yulesimon, pch=19, col='red')
plot(geom, pch=19, ylab='probability', xlab='x')
points(yulesimon, pch=19, col='red')
legend(x='topright', legend=c('Geometric', 'Yule-Simon'))
legend(x='topright', legend=c('Geometric', 'Yule-Simon'), col=c('black', 'red'))
plot(geom, pch=19, ylab='probability', xlab='x')
points(yulesimon, pch=19, col='red')
legend(x='topright', legend=c('Geometric', 'Yule-Simon'), col=c('black', 'red'))
legend(x='topright', legend=c('Geometric', 'Yule-Simon'), col=c('black', 'red'), pch=19)
plot(
geom,
pch=19,
ylab='probability',
xlab='x',
main='PMFs of Geometric and Yule-Simon Distributions'
)
points(
yulesimon,
pch=19,
col='red'
)
legend(
x='topright',
legend=c('Geometric', 'Yule-Simon'),
col=c('black', 'red'),
pch=19
)
# Q 78
qgamma(0.02, shape=10, rate=1/8, lower=F)
Auto <- read.table("data/Auto.data")
knitr::opts_chunk$set(echo = TRUE)
apply(USArrests, 2, rank)['Mississippi',]
?rank
setwd("~/academics/MATH189/homeworks")
### HW 6
library(ISLR2)
hitters = data(Hitters)
force(Hitters)
?complete.cases
hitters = hitters[, -c('League', 'Division', 'NewLeague')]
hitters = hitters[, !names %in% c('League', 'Division', 'NewLeague')]
hitters = hitters[, !names(hitters) %in% c('League', 'Division', 'NewLeague')]
hitters = data(Hitters)
hitters = hitters[, !names(hitters) %in% c('League', 'Division', 'NewLeague')]
data(Hitters)
Hitters = Hitters[, !names(Hitters) %in% c('League', 'Division', 'NewLeague')]
Hitters = Hitters[complete.cases(Hitters),]
dim(Hitters)
?sample
### HW 6 3 b
train = sample(1:nrow(Hitters), nrow(Hitters) * 0.8)
test = (-train)
Hitters.train = Hitters[train]
Hitters.train = Hitters[train,]
Hitters.test = Hitters[test,]
263*0.8
lm3 = lm(Salaray ~ ., data=Hitters.train)
lm3 = lm(Salary ~ ., data=Hitters.train)
summary(lm3)
?predict
lm3.predict = predict(lm3, newx=Hitters.test)
mean((lm3.predict - Hitters.test["Salary"])^2)
Hitters.test
Hitters.test$Salary
lm3.predict = predict(lm3, newx=Hitters.test)
mean((lm3.predict - Hitters.test$Salary)^2)
lm3.predict = predict(lm3, Hitters.test)
mean((lm3.predict - Hitters.test$Salary)^2)
sqrt(mean((lm3.predict - Hitters.test$Salary)^2))
summary(lm3)
### HW 6 3 b
set.seed(1)
train = sample(1:nrow(Hitters), nrow(Hitters) * 0.8)
test = (-train)
Hitters.train = Hitters[train,]
Hitters.test = Hitters[test,]
lm3 = lm(Salary ~ ., data=Hitters.train)
summary(lm3)
lm3.predict = predict(lm3, Hitters.test)
sqrt(mean((lm3.predict - Hitters.test$Salary)^2))
summary(lm3)
sessionInfo()
Hitters.train["Salary"]
Hitters.train[-"Salary"]
Hitters.train[!"Salary"]
## HW 6 4
lasso.mod = glmnet(Hitters.train[, !names(Hitters.train) %in% c("Salary")], Hitters.train$Salary, alpha=1)
## HW 6 4
library(glmnet)
## HW 6 4
install.packages("glmnet")
## HW 6 4
library(glmnet)
lasso.mod = glmnet(Hitters.train[, !names(Hitters.train) %in% c("Salary")], Hitters.train$Salary, alpha=1)
lasso.mod = glmnet(Hitters.train[, !names(Hitters.train) %in% c("Salary")], Hitters.train$Salary, alpha=1, lambda=grid)
plot(lasso.mod)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
x = Hitters[, !names(Hitters) %in% c("Salary")]
y = Hitters$Salary
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
grid = 10^seq(10, -2, length = 100)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
View(Hitters)
x = model.matrix(Salary ~ ., Hitters)[, -1]
y = Hitters$Salary
grid = 10^seq(10, -2, length = 100)
lasso.mod = glmnet(x[train], y[train], alpha=1, lambda=grid)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test,])
sqrt(mean((lasso.pred - y[test])^2))
coef(lasso.mod)
out = glmnet(x, y, alpha=1, lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)
lasso.coef
bestlam
sqrt(mean((lasso.pred - y[test])^2))
bestlam
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:20, ]
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:17, ]
lasso.coef
set.seed(1)
x = model.matrix(Salary ~ ., Hitters)[, -1]
y = Hitters$Salary
grid = 10^seq(10, -2, length = 100)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
out = glmnet(x, y, alpha=1, lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:17, ]
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test,])
sqrt(mean((lasso.pred - y[test])^2))
?set.seed
## HW 6 5
ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid)
plot(ridge.mod)
cv.out = cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
bestlam = ridge.cv.out$lambda.min
ridge.cv.out = cv.glmnet(x[train,], y[train], alpha=0)
plot(ridge.cv.out)
bestlam = ridge.cv.out$lambda.min
lasso.coef
set.seed(1)
set.seed(1)
x = model.matrix(Salary ~ ., Hitters)[, -1]
y = Hitters$Salary
grid = 10^seq(10, -2, length = 100)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
set.seed(1)
out = glmnet(x, y, alpha=1, lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:17, ]
lasso.coef
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test,])
sqrt(mean((lasso.pred - y[test])^2))
set.seed(1)
x = model.matrix(Salary ~ ., Hitters)[, -1]
y = Hitters$Salary
grid = 10^seq(10, -2, length = 100)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
out = glmnet(x, y, alpha=1, lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:17, ]
lasso.coef
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test,])
sqrt(mean((lasso.pred - y[test])^2))
## HW 6 5
ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid)
plot(ridge.mod)
ridge.cv.out = cv.glmnet(x[train,], y[train], alpha=0)
plot(ridge.cv.out)
ridge.bestlam = ridge.cv.out$lambda.min
ridge.bestlam
ridge.out = glmnet(x, y, alpha=0, lambda=grid)
ridge.coef = predict(ridge.out, type="coefficients", s=ridge.bestlam)[1:17, ]
ridge.coef
ridge.pred = predict(ridge.mod, s=bestlam, newx=x[test,])
sqrt(mean((ridge.pred - y[test])^2))
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
set.seed(1)
x = model.matrix(Salary ~ ., Hitters)[, -1]
y = Hitters$Salary
grid = 10^seq(10, -2, length = 100)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
out = glmnet(x, y, alpha=1, lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:17, ]
lasso.coef
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test,])
sqrt(mean((lasso.pred - y[test])^2))
sqrt(mean((ridge.pred - y[test])^2))
ridge.pred = predict(ridge.mod, s=ridge.bestlam, newx=x[test,])
sqrt(mean((ridge.pred - y[test])^2))
View(Hitters)
setwd("~/projects/dds_transgender/candus")
library(car)
library(flexmix)
library(MASS)
library(leaps)
# Load in data
q88 = read.csv("q88.csv", header=T)
q88 = q88[, !names(q88) %in% c("X", "SEX", "GENDER_IDENTITY", "CURRENT_SEX")]
q88$GEDUCATION = factor(q88$GEDUCATION)
q88$Q93 = factor(q88$Q93)
q88$RACE_RECODE_CAT5 = factor(q88$RACE_RECODE_CAT5)
q88$POVERTYCAT_I = factor(q88$POVERTYCAT_I)
q88logit = glm(Q88 ~ ., data=q88, family="binomial")
summary(q88logit)
# Feature Selection, Lasso
library(glmnet)
x = model.matrix(Q88 ~ ., q88sub)
# Forward Feature Selection, BIC, regsubsets
q88sub = q88[, !names(q88) %in% c("HINC_I", "HINC_I_means", "HINC_I_strat")]
x = model.matrix(Q88 ~ ., q88sub)
x
x[,-1]
x = model.matrix(Q88 ~ ., q88sub)[, -1]
y = q88sub$Q88
train = sample(1:nrow(q88sub), nrow(q88sub) * 0.8)
test = (-train)
grid = 10^seq(10, -2, length = 100)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
out = glmnet(x, y, alpha=1, lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:17, ]
lasso.coef
lasso.coef[lasso.coef!=0]
out
lasso.mod
lasso.coef[lasso.coef!=0]
# Forward & Backward Feature Selection, BIC
step_both_BIC = step(q88logit, direction="both",
scope=list(lower=Q88 ~ TRANS_CIS,
upper=Q88 ~ .^2), k=log(1364))
summary(step_both_BIC)
lasso.coef[lasso.coef!=0]
# Forward Feature Selection, BIC, regsubsets
q88sub = q88[, !names(q88) %in% c("HINC_I", "HINC_I_means", "HINC_I_strat")]
ffs = regsubsets(Q88 ~ ., data=q88sub, method="forward")
summary(ffs)
ffs$rss
coef(ffs, 4)
coef(ffs, 6)
bfs = regsubsets(Q88 ~ ., data=q88sub, method="backward")
summary(bfs)
coef(bfs, 6)
q88sublogit = glm(Q88 ~ ., data=q88sub, family="binomial")
summary(q88sublogit)
step_both_BIC_sub = step(q88sublogit, direction="both",
scope=list(lower=Q88 ~ TRANS_CIS,
upper=Q88 ~ .^2), k=log(1364))
summary(step_both_BIC_sub)
x = model.matrix(Q88 ~ ., q88s)[, -1]
y = q88$Q88
x = model.matrix(Q88 ~ ., q88)[, -1]
y = q88$Q88
train = sample(1:nrow(q88), nrow(q88) * 0.8)
test = (-train)
grid = 10^seq(10, -2, length = 100)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
out = glmnet(x, y, alpha=1, lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:17, ]
lasso.coef
lasso.coef[lasso.coef!=0]
setwd("~/projects/dds_transgender/candus")
library(car)
library(flexmix)
library(MASS)
library(leaps)
library(glmnet)
# Load in data
q88 = read.csv("q88.csv", header=T)
q88 = q88[, !names(q88) %in% c("X", "SEX", "GENDER_IDENTITY", "CURRENT_SEX")]
q88$GEDUCATION = factor(q88$GEDUCATION)
q88$Q93 = factor(q88$Q93)
q88$RACE_RECODE_CAT5 = factor(q88$RACE_RECODE_CAT5)
q88$POVERTYCAT_I = factor(q88$POVERTYCAT_I)
q88$HINC_I = factor(q88$HINC_I)
q88$HINC_I_strat = factor(q88$HINC_I_strat)
# remove the redundant income features --> use POVERTYCAT_I
q88sub = q88[, !names(q88) %in% c("HINC_I", "HINC_I_means", "HINC_I_strat")]
# all variables
q88logit = glm(Q88 ~ ., data=q88, family="binomial")
summary(q88logit)
setwd("~/projects/dds_transgender/candus")
library(car)
library(flexmix)
library(MASS)
library(leaps)
library(glmnet)
# Load in data
q88 = read.csv("q88.csv", header=T)
q88 = q88[, !names(q88) %in% c("X", "SEX", "GENDER_IDENTITY", "CURRENT_SEX")]
q88$GEDUCATION = factor(q88$GEDUCATION)
q88$Q93 = factor(q88$Q93)
q88$RACE_RECODE_CAT5 = factor(q88$RACE_RECODE_CAT5)
q88$POVERTYCAT_I = factor(q88$POVERTYCAT_I)
q88$HINC_I = factor(q88$HINC_I)
q88$HINC_I_strat = factor(q88$HINC_I_strat)
# remove the redundant income features --> use POVERTYCAT_I
q88sub = q88[, !names(q88) %in% c("HINC_I", "HINC_I_means", "HINC_I_strat")]
# all variables
q88logit = glm(Q88 ~ ., data=q88, family="binomial")
summary(q88logit)
# all variables, q88sub
q88sublogit = glm(Q88 ~ ., data=q88sub, family="binomial")
summary(q88sublogit)
# Forward & Backward Feature Selection, BIC
step_both_BIC = step(q88logit, direction="both",
scope=list(lower=Q88 ~ TRANS_CIS,
upper=Q88 ~ .^2), k=log(1364))
summary(step_both_BIC)
# Forward & Backward Feature Selection, BIC, q88sub
step_both_BIC_sub = step(q88sublogit, direction="both",
scope=list(lower=Q88 ~ TRANS_CIS,
upper=Q88 ~ .^2), k=log(1364))
summary(step_both_BIC_sub)
# Forward & Backward Feature Selection, BIC, regsubsets
ffs = regsubsets(Q88 ~ ., data=q88, method="forward")
summary(ffs)
ffs$rss
coef(ffs, 6)
bfs = regsubsets(Q88 ~ ., data=q88, method="backward")
summary(bfs)
coef(bfs, 6)
# Forward & Backward Feature Selection, BIC, regsubsets, q88sub
ffs.sub = regsubsets(Q88 ~ ., data=q88sub, method="forward")
summary(ffs.sub)
coef(ffs.sub, 6)
bfs.sub = regsubsets(Q88 ~ ., data=q88sub, method="backward")
summary(bfs.sub)
coef(bfs.sub, 6)
# Feature Selection, Lasso
x = model.matrix(Q88 ~ ., q88)[, -1]
y = q88$Q88
train = sample(1:nrow(q88), nrow(q88) * 0.8)
test = (-train)
grid = 10^seq(10, -2, length = 100)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
out = glmnet(x, y, alpha=1, lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:17, ]
lasso.coef
lasso.coef[lasso.coef!=0]
# Feature Selection, Lasso, q88sub
x.sub = model.matrix(Q88 ~ ., q88sub)[, -1]
y.sub = q88sub$Q88
lasso.mod.sub = glmnet(x.sub[train,], y.sub[train], alpha=1, lambda=grid)
plot(lasso.mod.sub)
cv.out.sub = cv.glmnet(x.sub[train,], y[train], alpha=1)
plot(cv.out.sub)
bestlam.sub = cv.out.sub$lambda.min
bestlam
out.sub = glmnet(x.sub, y.sub, alpha=1, lambda=grid)
lasso.coef.sub = predict(out.sub, type="coefficients", s=bestlam.sub)
lasso.coef.sub
lasso.coef.sub[lasso.coef.sub!=0]
lasso.coef.sub = predict(out.sub, type="coefficients", s=bestlam.sub)[1:20,]
lasso.coef.sub
lasso.coef.sub[lasso.coef.sub!=0]
lasso.coef.sub = predict(out.sub, type="coefficients", s=bestlam.sub)[1:ncol(x.sub),]
lasso.coef.sub
lasso.coef.sub[lasso.coef.sub!=0]
lasso.coef.sub = predict(out.sub, type="coefficients", s=bestlam.sub)#[1:ncol(x.sub),]
lasso.coef.sub
lasso.coef.sub = predict(out.sub, type="coefficients", s=bestlam.sub)[1:ncol(q88sub),]
lasso.coef.sub
lasso.coef.sub = predict(out.sub, type="coefficients", s=bestlam.sub)[1:ncol(x.sub)+1,]
lasso.coef.sub
lasso.coef.sub[lasso.coef.sub!=0]
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:ncol(x)+1,]
lasso.coef
lasso.coef[lasso.coef!=0]
lasso.coef.sub = predict(out.sub, type="coefficients", s=bestlam.sub)[1:ncol(x.sub)+1,]
lasso.coef.sub
lasso.coef.sub[lasso.coef.sub!=0]
plot(lasso.mod.sub)
plot(lasso.mod.sub, label=5)
plot(lasso.mod.sub, label=3)
plot(lasso.mod, label=3)
library(plotmo)
install.packages("plotmo")
library(plotmo)
plot_glmnet(lasso.mod, label=3)
plot_glmnet(lasso.mod)
plot(lasso.mod)
plot_glmnet(lasso.mod)
?plot_glmnet
plot_glmnet(lasso.mod, xlim=c(0,2))
plot_glmnet(lasso.mod, xvar="lambda", xlim=c(0,2))
plot_glmnet(lasso.mod, xlim=c(2,0))
plot_glmnet(lasso.mod)
plot_glmnet(lasso.mod, xlim=c(1, 0))
plot_glmnet(lasso.mod)
plot_glmnet(lasso.mod, xlim=c(0, -5))
plot_glmnet(lasso.mod, xlim=c(0,-5), label=3)
plot_glmnet(lasso.mod, xlim=c(0,-5))
bestlam
log(bestlam)
plot(cv.out)
plot_glmnet(lasso.mod.sub)
plot_glmnet(lasso.mod.sub, xlim=c(0, -5))
plot_glmnet(lasso.mod, xlim=c(0,-5))
plot_glmnet(lasso.mod.sub, xlim=c(0,-5))
step_both_BIC
step_both_BIC_sub
ffs
coef(ffs, 6)
coef(ffs, 10)
coef(ffs, 8)
step_both_BIC
coef(bfs, 8)
coef(ffs, 8)
lasso.coef
lasso.coef[lasso.coef!=0]
lasso.coef.sub[lasso.coef.sub!=0]
step_both_BIC_sub
step_both_BIC
step_both_BIC_sub
# refit logistic regression model using stepwise features
q88logit_stepwise = glm(Q88 ~ TRANS_CIS + AGE + Q93 + POVERTYCAT_I, data=q88sub, family="binomial")
summary(q88logit_stepwise)
# refit logistic regression model using lasso features
q88logit_lasso = glm(Q88 ~ Q88 ~ TRANS_CIS + AGE + RACE_RECODE_CAT5 + Q93 + GEDUCATION, data=q88sub, family="binomial")
# refit logistic regression model using lasso features
q88logit_lasso = glm(Q88 ~ TRANS_CIS + AGE + RACE_RECODE_CAT5 + Q93 + GEDUCATION, data=q88sub, family="binomial")
summary(q88logit_lasso)
BIC(q88logit_stepwise)
BIC(q88logit_lasso)
View(q88)
lasso.coef.sub[lasso.coef.sub!=0]
bestlam.sub = cv.out.sub$lambda.min
bestlam
log(0.01675855)
